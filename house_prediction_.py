# -*- coding: utf-8 -*-
"""House prediction .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q-ShMp7Yp8y--LiY99d3NODuoEjKhG4L
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/Ajayjarupula/House-Price-Prediction
# %cd House-Price-Prediction

!pip install scikit-learn joblib fastapi uvicorn

import os
os.makedirs("src", exist_ok=True)  # Creates the src directory if it does not exist

# Commented out IPython magic to ensure Python compatibility.
# %%writefile src/preprocess.py
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# from sklearn.impute import SimpleImputer
# from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer
# from sklearn.datasets import fetch_california_housing
# import joblib
# 
# def load_data():
#     """Load California housing dataset and return a DataFrame."""
#     data = fetch_california_housing()
#     df = pd.DataFrame(data.data, columns=data.feature_names)
#     df["Price"] = data.target
#     return df
# 
# def preprocess_data(df):
#     """Preprocess data: handle missing values and scale numerical features."""
#     numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
#     numerical_cols.remove("Price")  # Exclude target variable
# 
#     num_pipeline = Pipeline([
#         ("imputer", SimpleImputer(strategy="median")),
#         ("scaler", StandardScaler())
#     ])
# 
#     preprocessor = ColumnTransformer([
#         ("num", num_pipeline, numerical_cols)
#     ])
# 
#     X = df.drop(columns=["Price"])
#     y = df["Price"]
# 
#     X_processed = preprocessor.fit_transform(X)
# 
#     # Save the preprocessor
#     joblib.dump(preprocessor, "src/preprocessor.pkl")
# 
#     return X_processed, y
# 
# if __name__ == "__main__":
#     df = load_data()
#     X, y = preprocess_data(df)
#     print("Preprocessing complete. Data ready for model training.")
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile src/train.py
# from preprocess import load_data, preprocess_data
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.ensemble import RandomForestRegressor
# from sklearn.metrics import mean_squared_error
# from sklearn.metrics import mean_absolute_error, r2_score
# import joblib
# import numpy as np
# 
# # Load preprocessed data
# df = load_data()
# X, y = preprocess_data(df)
# 
# # Train-test split
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 
# # Define model and hyperparameter tuning
# param_grid = {
#     "n_estimators": [50, 100, 200],
#     "max_depth": [None, 10, 20]
# }
# 
# rf = RandomForestRegressor(random_state=42)
# grid_search = GridSearchCV(rf, param_grid, cv=3, scoring="r2", n_jobs=-1)
# grid_search.fit(X_train, y_train)
# 
# best_model = grid_search.best_estimator_
# 
# # Evaluate the model
# y_pred = best_model.predict(X_test)
# print(f"MAE: {mean_absolute_error(y_test, y_pred)}")
# rmse = np.sqrt(mean_squared_error(y_test, y_pred))
# print(f"RMSE: {rmse}")
# print(f"RÂ² Score: {r2_score(y_test, y_pred)}")
# 
# # Save the trained model
# joblib.dump(best_model, "src/model.pkl")
# print("Model saved successfully as model.pkl.")
#

!python src/train.py